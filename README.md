# ğŸ”§ Tractian Knowledge Base - RAG System

A **Retrieval-Augmented Generation (RAG)** system that allows users to upload PDF documents and ask questions about their contents. Built with FastAPI, FAISS, and OpenAI.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit   â”‚â”€â”€â”€â”€â–¶â”‚              FastAPI Backend              â”‚
â”‚  Frontend    â”‚â—€â”€â”€â”€â”€â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ PDF Process  â”‚   â”‚  LLM Service     â”‚  â”‚
                    â”‚  â”‚ - Extract    â”‚   â”‚  - OpenAI GPT-4  â”‚  â”‚
                    â”‚  â”‚ - Chunk      â”‚   â”‚  - Prompt Eng.   â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â”‚         â”‚                    â”‚             â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚          Vector Store (FAISS)         â”‚ â”‚
                    â”‚  â”‚  - Embeddings (sentence-transformers) â”‚ â”‚
                    â”‚  â”‚  - Cosine Similarity Search           â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- OpenAI API key
- Gemini Key

### 1. Clone & Install

```bash
git clone https://github.com/fabriciojesus/tractian-knowledge-base.git
cd Tractian_KB

# This will create a virtual environment and install dependencies
make install
```

### 2. Configure

```bash
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
```

### 3. Run

```bash
# Start the API
make run-api

# In another terminal, start the frontend
make run-frontend
```

- **API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs
- **Frontend**: http://localhost:8501

## ğŸ“¡ API Specification

### POST /documents

Upload PDF documents to be indexed.

```bash
curl -X POST "http://localhost:8000/documents" \
  -F "files=@document1.pdf" \
  -F "files=@document2.pdf"
```

**Response:**
```json
{
  "message": "Documents processed successfully",
  "documents_indexed": 2,
  "total_chunks": 128
}
```

### POST /question

Ask a question about uploaded documents.

```bash
curl -X POST "http://localhost:8000/question" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the power consumption of the motor?"}'
```

**Response:**
```json
{
  "answer": "The motor's power consumption is 2.3 kW.",
  "references": [
    "the motor xxx requires 2.3kw to operate at a 60hz line frequency"
  ]
}
```

### GET /health

Health check endpoint.

```bash
curl http://localhost:8000/health
```

## ğŸ³ Docker

```bash
# Build and start all services
make docker-build
make docker-up

# View logs
make docker-logs

# Stop
make docker-down
```

## ğŸ§ª Testing

```bash
# Run all tests with coverage
make test

# Run unit tests only
make test-unit

# Run API tests only
make test-api
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py        # API endpoints
â”‚   â”‚   â””â”€â”€ models.py        # Pydantic models
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ pdf_processor.py # PDF extraction & chunking
â”‚       â”œâ”€â”€ embeddings.py    # Embedding generation
â”‚       â”œâ”€â”€ vector_store.py  # ChromaDB operations
â”‚       â””â”€â”€ llm_service.py   # LLM integration
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ streamlit_app.py     # Streamlit UI
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py          # API integration tests
â”‚   â””â”€â”€ test_services.py     # Unit tests
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

All configuration is managed via environment variables (`.env` file):

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | - | OpenAI API key (required) |
| `OPENAI_MODEL` | `gpt-4` | LLM model to use |
| `EMBEDDING_MODEL` | `all-MiniLM-L6-v2` | Sentence transformer model |
| `CHUNK_SIZE` | `1000` | Text chunk size in characters |
| `CHUNK_OVERLAP` | `200` | Overlap between chunks |
| `TOP_K_RESULTS` | `3` | Number of chunks for context |
| `LLM_TEMPERATURE` | `0.1` | LLM temperature (lower = more deterministic) |

## ğŸ§  Technical Decisions

### Why FAISS?
Facebook AI Similarity Search is extremely fast for vector similarity queries. Used with normalized vectors and inner product for cosine similarity. Persists to disk with JSON metadata â€” no external server needed.

### Why sentence-transformers (all-MiniLM-L6-v2)?
Fast, lightweight embedding model with good multilingual support. Runs locally without API calls, reducing latency and cost.

### Why Recursive Character Text Splitter?
Preserves semantic meaning by splitting on natural boundaries (paragraphs â†’ sentences â†’ words). The 200-character overlap ensures context continuity across chunks.

### Why pdfplumber?
Superior text extraction compared to PyPDF2, especially for documents with tables, columns, and complex layouts â€” common in industrial/engineering PDFs.

## ğŸ“ License

MIT
